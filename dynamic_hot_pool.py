"""
PC28 åŠ¨æ€çƒ­é—¨å·ç æ± ç”Ÿæˆå™¨
æ¯ä¸€æœŸå®æ—¶æ›´æ–°ï¼ŒåŸºäºä¸‰å¹´æ»šåŠ¨çª—å£ç»Ÿè®¡Top 875çƒ­é—¨å·ç 
"""
import pandas as pd
from datetime import datetime, timedelta
from typing import Set, List, Tuple
import os


class DynamicHotPool:
    """åŠ¨æ€å·ç æ± ç”Ÿæˆå™¨"""
    
    def __init__(self, data_file: str):
        """
        åˆå§‹åŒ–å¹¶åŠ è½½å†å²æ•°æ®
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆ.txtæˆ–.xlsxï¼‰
        """
        self.data_file = data_file
        self.df = None
        self._cache = {}  # ç¼“å­˜: {(date_str, top_n): (hot_pool, counts)}
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        if self.data_file.endswith('.txt'):
            self._load_txt_data()
        elif self.data_file.endswith('.xlsx'):
            self._load_excel_data()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {self.data_file}")
        
        print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.df)} æ¡è®°å½•")
    
    def _load_txt_data(self):
        """ä»txtæ–‡ä»¶åŠ è½½æ•°æ®"""
        data = []
        with open(self.data_file, 'r', encoding='utf-8') as f:
            # è·³è¿‡æ ‡é¢˜è¡Œ
            next(f); next(f); next(f)
            
            for line in f:
                if not line.strip():
                    continue
                
                parsed = self._parse_line(line)
                if parsed:
                    data.append(parsed)
        
        self.df = pd.DataFrame(data)
        # æŒ‰æ—¶é—´æ’åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
        self.df = self.df.sort_values('datetime').reset_index(drop=True)
    
    def _load_excel_data(self):
        """ä»Excelæ–‡ä»¶åŠ è½½æ•°æ®"""
        # è¯»å–Excel
        df = pd.read_excel(self.data_file)
        
        # å‡è®¾Excelæœ‰ 'æ—¶é—´' å’Œ 'å¼€å¥–å·' åˆ—
        # æ ¹æ®å®é™…Excelç»“æ„è°ƒæ•´
        data = []
        for _, row in df.iterrows():
            # éœ€è¦æ ¹æ®å®é™…Excelæ ¼å¼è§£æ
            # è¿™é‡Œæä¾›ä¸€ä¸ªç¤ºä¾‹æ¡†æ¶
            pass
        
        self.df = pd.DataFrame(data)
        self.df = self.df.sort_values('datetime').reset_index(drop=True)
    
    def _parse_line(self, line: str) -> dict:
        """è§£ætxtæ–‡ä»¶ä¸­çš„ä¸€è¡Œæ•°æ®"""
        try:
            parts = line.split()
            if len(parts) < 8:
                return None
            
            # è§£ææ—¥æœŸå’Œæ—¶é—´
            date_str = parts[0]
            time_str = parts[1]
            datetime_str = f"{date_str} {time_str}"
            dt = datetime.strptime(datetime_str, "%Y-%m-%d %H:%M:%S")
            
            # è§£æå·ç 
            n1 = parts[3]
            n2 = parts[5]
            n3 = parts[7]
            
            if not (n1.isdigit() and n2.isdigit() and n3.isdigit()):
                return None
            
            return {
                'datetime': dt,
                'numbers': f"{n1},{n2},{n3}"
            }
        except Exception as e:
            return None
    
    def get_hot_pool(self, current_time: datetime, top_n: int = 875) -> Tuple[Set[str], pd.Series]:
        """
        è·å–å½“å‰æ—¶åˆ»çš„çƒ­é—¨å·ç æ± ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰
        
        Args:
            current_time: å½“å‰æ—¶é—´ç‚¹
            top_n: å–å‰Nä¸ªçƒ­é—¨å·ç ï¼Œé»˜è®¤875
        
        Returns:
            (hot_pool, frequency_stats): çƒ­é—¨å·ç é›†åˆ å’Œ é¢‘æ¬¡ç»Ÿè®¡
        """
        # ğŸ”¥ ç¼“å­˜ä¼˜åŒ–ï¼šæŒ‰æ—¥æœŸç¼“å­˜ï¼ˆåŒä¸€å¤©çš„ä¸åŒæ—¶åˆ»ä½¿ç”¨åŒä¸€ç¼“å­˜ï¼‰
        cache_key = (current_time.date().isoformat(), top_n)
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # è®¡ç®—ä¸‰å¹´å‰çš„æ—¶é—´ç‚¹
        three_years_ago = current_time - timedelta(days=3*365)
        
        # ç­›é€‰æ•°æ®çª—å£: [ä¸‰å¹´å‰, å½“å‰æ—¶åˆ»]
        mask = (self.df['datetime'] >= three_years_ago) & (self.df['datetime'] <= current_time)
        window_df = self.df[mask]
        
        if len(window_df) == 0:
            print(f"è­¦å‘Š: åœ¨æ—¶é—´çª—å£ [{three_years_ago}] åˆ° [{current_time}] å†…æ²¡æœ‰æ‰¾åˆ°æ•°æ®")
            return set(), pd.Series()
        
        # ç»Ÿè®¡é¢‘æ¬¡
        counts = window_df['numbers'].value_counts()
        
        # å–Top N
        top_numbers = counts.head(top_n).index.tolist()
        hot_pool = set(top_numbers)
        
        # ç¼“å­˜ç»“æœ
        self._cache[cache_key] = (hot_pool, counts)
        
        return hot_pool, counts
    
    def get_pool_stats(self, current_time: datetime, top_n: int = 875) -> dict:
        """
        è·å–å·ç æ± ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            current_time: å½“å‰æ—¶é—´ç‚¹
            top_n: å–å‰Nä¸ªçƒ­é—¨å·ç 
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        hot_pool, counts = self.get_hot_pool(current_time, top_n)
        
        three_years_ago = current_time - timedelta(days=3*365)
        mask = (self.df['datetime'] >= three_years_ago) & (self.df['datetime'] <= current_time)
        window_df = self.df[mask]
        
        stats = {
            'current_time': current_time,
            'window_start': three_years_ago,
            'window_records': len(window_df),
            'unique_numbers': len(counts),
            'hot_pool_size': len(hot_pool),
            'cutoff_frequency': counts.iloc[top_n-1] if len(counts) >= top_n else None,
            'max_frequency': counts.iloc[0] if len(counts) > 0 else None,
            'min_frequency': counts.iloc[-1] if len(counts) > 0 else None
        }
        
        return stats
    
    def add_new_record(self, dt: datetime, numbers: str):
        """
        å¢é‡æ·»åŠ æ–°è®°å½•ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        
        Args:
            dt: å¼€å¥–æ—¶é—´
            numbers: å·ç ï¼Œæ ¼å¼ "2,3,3"
        """
        new_row = pd.DataFrame([{
            'datetime': dt,
            'numbers': numbers
        }])
        self.df = pd.concat([self.df, new_row], ignore_index=True)
        self.df = self.df.sort_values('datetime').reset_index(drop=True)
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜ï¼ˆå½“æ•°æ®æ›´æ–°æ—¶è°ƒç”¨ï¼‰"""
        self._cache.clear()
        print("âœ… ç¼“å­˜å·²æ¸…ç©º")
    
    def get_cache_info(self) -> dict:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        return {
            'cache_size': len(self._cache),
            'cached_dates': [key[0] for key in self._cache.keys()]
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºåŠ¨æ€å·ç æ± 
    pool = DynamicHotPool(r"c:\Users\tiancheng\Desktop\å½©\æ•°æ®\pc28_data_repaired.txt")
    
    # æ¨¡æ‹Ÿä¸åŒæ—¶é—´ç‚¹
    test_times = [
        datetime(2026, 1, 11, 0, 0, 0),    # ä»Šå¤©0ç‚¹
        datetime(2026, 1, 11, 12, 0, 0),   # ä»Šå¤©ä¸­åˆ
        datetime(2026, 1, 11, 19, 16, 0),  # å½“å‰æ—¶åˆ»
    ]
    
    for test_time in test_times:
        print(f"\n{'='*60}")
        print(f"æ¨¡æ‹Ÿæ—¶é—´: {test_time}")
        print(f"{'='*60}")
        
        stats = pool.get_pool_stats(test_time, top_n=875)
        
        print(f"æ•°æ®çª—å£: {stats['window_start']} åˆ° {stats['current_time']}")
        print(f"çª—å£å†…è®°å½•æ•°: {stats['window_records']}")
        print(f"å”¯ä¸€å·ç ç§ç±»: {stats['unique_numbers']}")
        print(f"çƒ­é—¨æ± å¤§å°: {stats['hot_pool_size']}")
        print(f"ç¬¬875åé¢‘æ¬¡: {stats['cutoff_frequency']}")
        print(f"æœ€é«˜é¢‘æ¬¡: {stats['max_frequency']}")
        print(f"æœ€ä½é¢‘æ¬¡: {stats['min_frequency']}")
        
        # è·å–çƒ­é—¨å·ç æ± 
        hot_pool, counts = pool.get_hot_pool(test_time, top_n=875)
        print(f"\nTop 10 çƒ­é—¨å·ç :")
        for i, (num, freq) in enumerate(counts.head(10).items(), 1):
            print(f"  {i}. {num}: {freq}æ¬¡")
